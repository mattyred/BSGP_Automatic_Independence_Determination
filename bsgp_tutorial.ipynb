{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSGP model Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bsgp.models import RegressionModel\n",
    "import argparse\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import  TensorDataset\n",
    "from scipy.stats import bernoulli as np_bern\n",
    "from bsgp.dgp_model import DGP\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "from bsgp.kernels import SquaredExponential as BgpSE\n",
    "from bsgp.likelihoods import Gaussian\n",
    "import tensorflow as tf\n",
    "\n",
    "PRIORS = ['uniform', 'normal', 'determinantal', 'strauss']\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Boston dataset as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_regression import create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: 404 x 13\n",
      "Shape of Y_train: 404 x 1\n",
      "Shape of X_test:  102 x 13\n",
      "Shape of Y_test:  102 x 1\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train,  X_test, Y_test, Y_train_mean, Y_train_std = create_dataset(\"Boston\", 0)\n",
    "print('Shape of X_train: %d x %d'%(X_train.shape[0],X_train.shape[1]))\n",
    "print('Shape of Y_train: %d x %d'%(Y_train.shape[0],Y_train.shape[1]))\n",
    "print('Shape of X_test:  %d x %d'%(X_test.shape[0],X_test.shape[1]))\n",
    "print('Shape of Y_test:  %d x %d'%(Y_test.shape[0],Y_test.shape[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RegressionModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the properties (ARGS) of the model (the ones that are specified with the command line arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inducing points: 100\n"
     ]
    }
   ],
   "source": [
    "prior_type = 'normal' # prior / strauss / determinantal / uniform\n",
    "class ARGS:\n",
    "    num_inducing = 100\n",
    "    iterations = 10000\n",
    "    minibatch_size = 100\n",
    "    window_size = 64\n",
    "    num_posterior_samples = 100\n",
    "    posterior_sample_spacing = 50\n",
    "    full_cov = True\n",
    "    n_layers = 1\n",
    "    prior_type = None\n",
    "    logdir = '/tmp/'\n",
    "ARGS = ARGS()\n",
    "ARGS.num_inducing = 100\n",
    "ARGS.minibatch_size = 10\n",
    "ARGS.iterations = 1000\n",
    "ARGS.n_layers = 1\n",
    "ARGS.num_posterior_samples = 100\n",
    "ARGS.prior_type = prior_type\n",
    "ARGS.full_cov = False\n",
    "ARGS.posterior_sample_spacing = 32\n",
    "print('Number of inducing points: %d' % ARGS.num_inducing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the RegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lik = Gaussian(np.var(Y_train, 0)) # define a Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = Y_train\n",
    "if len(Y.shape) == 1:\n",
    "    Y = Y[:, None]\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Layer: 0 - Lengthscales:  13.0\n"
     ]
    }
   ],
   "source": [
    "kerns = []\n",
    "for i in range(ARGS.n_layers):\n",
    "    output_dim = 196 if i >= 1 and X_train.shape[1] > 700 else X_train.shape[1]\n",
    "    lengthscales = float(min(X_train.shape[1], output_dim))\n",
    "    print('#Layer: %d - Lengthscales: '%(i), lengthscales)\n",
    "    kerns.append(BgpSE(output_dim, ARD=True, lengthscales=lengthscales**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch size: 10\n"
     ]
    }
   ],
   "source": [
    "mb_size = ARGS.minibatch_size if X_train.shape[0] > ARGS.minibatch_size else X_train.shape[0]\n",
    "print('Minibatch size: %d'%(mb_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Hamiltonian Monte Carlo Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_placeholder = tf.compat.v1.placeholder(tf.float64, shape=[None, X_train.shape[1]])\n",
    "Y_placeholder = tf.compat.v1.placeholder(tf.float64, shape=[None, Y_train.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = 0\n",
    "def get_minibatch():\n",
    "    N = X_train.shape[0]\n",
    "    minibatch_size = mb_size\n",
    "    assert N >= minibatch_size\n",
    "    if N == minibatch_size:\n",
    "        return X_train, Y_train\n",
    "\n",
    "    if N < data_iter + minibatch_size:\n",
    "        shuffle = np.random.permutation(N)\n",
    "        X = X_train[shuffle, :]\n",
    "        Y = Y_train[shuffle, :]\n",
    "        data_iter = 0\n",
    "\n",
    "    X_batch = X[data_iter:data_iter + minibatch_size, :]\n",
    "    Y_batch = Y[data_iter:data_iter + minibatch_size, :]\n",
    "    data_iter += minibatch_size\n",
    "    return X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc_step():\n",
    "    X_batch, Y_batch = get_minibatch()\n",
    "    feed_dict = {X_placeholder: X_batch, Y_placeholder: Y_batch}\n",
    "    self.session.run(self.burn_in_op, feed_dict=feed_dict)\n",
    "    for j in range(10):\n",
    "        X_batch, Y_batch = self.get_minibatch()\n",
    "        feed_dict = {self.X_placeholder: X_batch, self.Y_placeholder: Y_batch}\n",
    "        self.session.run(self.burn_in_op, feed_dict=feed_dict)\n",
    "        self.session.run((self.sample_op), feed_dict=feed_dict)\n",
    "\n",
    "    values = self.session.run((self.vars))\n",
    "    sample = {}\n",
    "    for var, value in zip(self.vars, values):\n",
    "        sample[var] = value\n",
    "    self.window.append(sample)\n",
    "    if len(self.window) > self.window_size:\n",
    "        self.window = self.window[-self.window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "for _ in range(self.ARGS.iterations):\n",
    "    global_step += 1\n",
    "    self.model.sghmc_step()\n",
    "    if ARGS.prior_type == \"determinantal\":\n",
    "        model.reset_Lm()\n",
    "        self.model.train_hypers() if hasattr(self.model, 'hyper_train_op') else None\n",
    "        if _ % 250 == 1:\n",
    "            marginal_ll = self.model.print_sample_performance()\n",
    "            # writer.add_scalar('optimisation/marginal_likelihood', marginal_ll*len(X), self.global_step)\n",
    "            print('TRAIN | iter = %6d      sample marginal LL = %5.2f' % (_, marginal_ll))\n",
    "            # Test with previous samples with Xtest and Ytest are both not None\n",
    "            if not (Xtest is None or Ytest is None or Ystd is None):\n",
    "                ms, vs = self.model.predict_y(Xtest, len(self.model.window), posterior=False)\n",
    "                logps = norm.logpdf(np.repeat(Ytest[None, :, :]*Ystd, len(self.model.window), axis=0), ms*Ystd, np.sqrt(vs)*Ystd)\n",
    "                mnll = -np.mean(logsumexp(logps, axis=0) - np.log(len(self.model.window)))\n",
    "                # writer.add_scalar('test/predictive_nloglikelihood', mnll, self.global_step)\n",
    "                print('TEST  | iter = %6d       MNLL = %5.2f' % (_, mnll))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
